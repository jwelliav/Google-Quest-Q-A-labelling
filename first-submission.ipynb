{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupKFold\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import os\n",
    "from scipy.stats import spearmanr\n",
    "from math import floor, ceil\n",
    "from transformers import *\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import gc\n",
    "import pickle  \n",
    "import random\n",
    "import keras\n",
    "import tensorflow_hub as hub\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "from scipy.stats import spearmanr, rankdata\n",
    "from os.path import join as path_join\n",
    "from numpy.random import seed\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import MultiTaskElasticNet\n",
    "\n",
    "seed(42)\n",
    "tf.random.set_seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read data and tokenizer\n",
    "\n",
    "Read tokenizer and data, as well as defining the maximum sequence length that will be used for the input to Bert (maximum is usually 512 tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape = (6079, 41)\n",
      "test shape = (476, 11)\n",
      "\n",
      "output categories:\n",
      "\t ['question_asker_intent_understanding', 'question_body_critical', 'question_conversational', 'question_expect_short_answer', 'question_fact_seeking', 'question_has_commonly_accepted_answer', 'question_interestingness_others', 'question_interestingness_self', 'question_multi_intent', 'question_not_really_a_question', 'question_opinion_seeking', 'question_type_choice', 'question_type_compare', 'question_type_consequence', 'question_type_definition', 'question_type_entity', 'question_type_instructions', 'question_type_procedure', 'question_type_reason_explanation', 'question_type_spelling', 'question_well_written', 'answer_helpful', 'answer_level_of_information', 'answer_plausible', 'answer_relevance', 'answer_satisfaction', 'answer_type_instructions', 'answer_type_procedure', 'answer_type_reason_explanation', 'answer_well_written']\n",
      "\n",
      "input categories:\n",
      "\t ['question_title', 'question_body', 'answer']\n"
     ]
    }
   ],
   "source": [
    "PATH = '../input/google-quest-challenge/'\n",
    "\n",
    "#BERT_PATH = '../input/bert-base-uncased-huggingface-transformer/'\n",
    "#tokenizer = BertTokenizer.from_pretrained(BERT_PATH+'bert-base-uncased-vocab.txt')\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "\n",
    "df_train = pd.read_csv(PATH+'train.csv')\n",
    "df_test = pd.read_csv(PATH+'test.csv')\n",
    "df_sub = pd.read_csv(PATH+'sample_submission.csv')\n",
    "print('train shape =', df_train.shape)\n",
    "print('test shape =', df_test.shape)\n",
    "\n",
    "output_categories = list(df_train.columns[11:])\n",
    "input_categories = list(df_train.columns[[1,2,5]])\n",
    "print('\\noutput categories:\\n\\t', output_categories)\n",
    "print('\\ninput categories:\\n\\t', input_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Preprocessing functions\n",
    "\n",
    "These are some functions that will be used to preprocess the raw text data into useable Bert inputs.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_to_transformer_inputs(title, question, answer, tokenizer, max_sequence_length):\n",
    "    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n",
    "    \n",
    "    def return_id(str1, str2, truncation_strategy, length):\n",
    "\n",
    "        inputs = tokenizer.encode_plus(str1, str2,\n",
    "            add_special_tokens=True,\n",
    "            max_length=length,\n",
    "            truncation_strategy=truncation_strategy)\n",
    "        \n",
    "        input_ids =  inputs[\"input_ids\"]\n",
    "        input_masks = [1] * len(input_ids)\n",
    "        input_segments = inputs[\"token_type_ids\"]\n",
    "        padding_length = length - len(input_ids)\n",
    "        padding_id = tokenizer.pad_token_id\n",
    "        input_ids = input_ids + ([padding_id] * padding_length)\n",
    "        input_masks = input_masks + ([0] * padding_length)\n",
    "        input_segments = input_segments + ([0] * padding_length)\n",
    "        \n",
    "        return [input_ids, input_masks, input_segments]\n",
    "    \n",
    "    input_ids_q, input_masks_q, input_segments_q = return_id(\n",
    "        title + ' ' + question, None, 'longest_first', max_sequence_length)\n",
    "    \n",
    "    input_ids_a, input_masks_a, input_segments_a = return_id(\n",
    "        answer, None, 'longest_first', max_sequence_length)\n",
    "    \n",
    "    return [input_ids_q, input_masks_q, input_segments_q,\n",
    "            input_ids_a, input_masks_a, input_segments_a]\n",
    "\n",
    "def compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n",
    "    input_ids_q, input_masks_q, input_segments_q = [], [], []\n",
    "    input_ids_a, input_masks_a, input_segments_a = [], [], []\n",
    "    for _, instance in tqdm(df[columns].iterrows()):\n",
    "        t, q, a = instance.question_title, instance.question_body, instance.answer\n",
    "\n",
    "        ids_q, masks_q, segments_q, ids_a, masks_a, segments_a = \\\n",
    "        _convert_to_transformer_inputs(t, q, a, tokenizer, max_sequence_length)\n",
    "        \n",
    "        input_ids_q.append(ids_q)\n",
    "        input_masks_q.append(masks_q)\n",
    "        input_segments_q.append(segments_q)\n",
    "\n",
    "        input_ids_a.append(ids_a)\n",
    "        input_masks_a.append(masks_a)\n",
    "        input_segments_a.append(segments_a)\n",
    "        \n",
    "    return [np.asarray(input_ids_q, dtype=np.int32), \n",
    "            np.asarray(input_masks_q, dtype=np.int32), \n",
    "            np.asarray(input_segments_q, dtype=np.int32),\n",
    "            np.asarray(input_ids_a, dtype=np.int32), \n",
    "            np.asarray(input_masks_a, dtype=np.int32), \n",
    "            np.asarray(input_segments_a, dtype=np.int32)]\n",
    "\n",
    "def compute_output_arrays(df, columns):\n",
    "    return np.asarray(df[columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Create model\n",
    "\n",
    "`compute_spearmanr()` is used to compute the competition metric for the validation set\n",
    "<br><br>\n",
    "`create_model()` contains the actual architecture that will be used to finetune BERT to our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_spearmanr_ignore_nan(trues, preds):\n",
    "    rhos = []\n",
    "    for tcol, pcol in zip(np.transpose(trues), np.transpose(preds)):\n",
    "        rhos.append(spearmanr(tcol, pcol).correlation)\n",
    "    return np.nanmean(rhos)\n",
    "\n",
    "def create_model():\n",
    "    q_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    q_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    q_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    a_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
    "    \n",
    "    model_class = TFAutoModel\n",
    "    bert_model = model_class.from_pretrained('bert-base-uncased')\n",
    "    # caution: when using e.g. XLNet, XLNetConfig() will automatically use xlnet-large config\n",
    "    \n",
    "    # normally \".from_pretrained('bert-base-uncased')\", but because of no internet, the \n",
    "    # pretrained model has been downloaded manually and uploaded to kaggle. \n",
    "#   bert_model = TFBertModel.from_pretrained(\n",
    "#       BERT_PATH+'bert-base-uncased-tf_model.h5', config=config)\n",
    "    \n",
    "    # if config.output_hidden_states = True, obtain hidden states via bert_model(...)[-1]\n",
    "    q_embedding = bert_model(q_id, attention_mask=q_mask, token_type_ids=q_atn)[0]\n",
    "    a_embedding = bert_model(a_id, attention_mask=a_mask, token_type_ids=a_atn)[0]\n",
    "    \n",
    "    q = tf.keras.layers.GlobalAveragePooling1D()(q_embedding)\n",
    "    a = tf.keras.layers.GlobalAveragePooling1D()(a_embedding)\n",
    "    \n",
    "    x = tf.keras.layers.Concatenate()([q, a])\n",
    "    \n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(30, activation='sigmoid')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[q_id, q_mask, q_atn, a_id, a_mask, a_atn,], outputs=x)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Obtain inputs and targets, as well as the indices of the train/validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c5b64599b7e4dc3847c624f68108983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "outputs = compute_output_arrays(df_train, output_categories)\n",
    "inputs = compute_input_arrays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n",
    "#test_inputs = compute_input_arrays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Training, validation and testing\n",
    "\n",
    "Loops over the folds in gkf and trains each fold for 3 epochs --- with a learning rate of 3e-5 and batch_size of 6. A simple binary crossentropy is used as the objective-/loss-function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compatible with tensorflow backend\n",
    "class SpearmanRhoCallback(Callback):\n",
    "    def __init__(self, training_data, validation_data, patience, model_name):\n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "        \n",
    "        self.patience = patience\n",
    "        self.value = -1\n",
    "        self.bad_epochs = 0\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred_val = self.model.predict(self.x_val)\n",
    "        rho_val = np.mean([spearmanr(self.y_val[:, ind], y_pred_val[:, ind] + np.random.normal(0, 1e-7, y_pred_val.shape[0])).correlation for ind in range(y_pred_val.shape[1])])\n",
    "        if rho_val >= self.value:\n",
    "            self.value = rho_val\n",
    "            self.bad_epochs = 0\n",
    "            self.model.save_weights(self.model_name)\n",
    "        else:\n",
    "            self.bad_epochs += 1\n",
    "        if self.bad_epochs >= self.patience:\n",
    "            print(\"Epoch %05d: early stopping Threshold\" % epoch)\n",
    "            self.model.stop_training = True\n",
    "            \n",
    "        print('\\rval_spearman-rho: %s' % (str(round(rho_val, 4))), end=100*' '+'\\n')\n",
    "        return rho_val\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4863 samples, validate on 1216 samples\n",
      "Epoch 1/3\n",
      "val_spearman-rho: 0.3644                                                                                                    \n",
      "4863/4863 [==============================] - 814s 167ms/sample - loss: 0.3963 - val_loss: 0.3755\n",
      "Epoch 2/3\n",
      "val_spearman-rho: 0.3846                                                                                                    \n",
      "4863/4863 [==============================] - 767s 158ms/sample - loss: 0.3660 - val_loss: 0.3742\n",
      "Epoch 3/3\n",
      "val_spearman-rho: 0.3891                                                                                                    \n",
      "4863/4863 [==============================] - 767s 158ms/sample - loss: 0.3523 - val_loss: 0.3747\n",
      "Train on 4863 samples, validate on 1216 samples\n",
      "Epoch 1/3\n",
      "val_spearman-rho: 0.3601                                                                                                    \n",
      "4863/4863 [==============================] - 808s 166ms/sample - loss: 0.3976 - val_loss: 0.3759\n",
      "Epoch 2/3\n",
      "val_spearman-rho: 0.3782                                                                                                    \n",
      "4863/4863 [==============================] - 767s 158ms/sample - loss: 0.3665 - val_loss: 0.3712\n",
      "Epoch 3/3\n",
      "val_spearman-rho: 0.3851                                                                                                    \n",
      "4863/4863 [==============================] - 767s 158ms/sample - loss: 0.3520 - val_loss: 0.3715\n",
      "Train on 4863 samples, validate on 1216 samples\n",
      "Epoch 1/3\n",
      "val_spearman-rho: 0.3704                                                                                                    \n",
      "4863/4863 [==============================] - 809s 166ms/sample - loss: 0.4005 - val_loss: 0.3715\n",
      "Epoch 2/3\n",
      "val_spearman-rho: 0.3946                                                                                                    \n",
      "4863/4863 [==============================] - 767s 158ms/sample - loss: 0.3683 - val_loss: 0.3661\n",
      "Epoch 3/3\n",
      "val_spearman-rho: 0.3934                                                                                                    \n",
      "4863/4863 [==============================] - 766s 158ms/sample - loss: 0.3535 - val_loss: 0.3727\n",
      "Train on 4863 samples, validate on 1216 samples\n",
      "Epoch 1/3\n",
      "val_spearman-rho: 0.3589                                                                                                    \n",
      "4863/4863 [==============================] - 809s 166ms/sample - loss: 0.4001 - val_loss: 0.3728\n",
      "Epoch 2/3\n",
      "val_spearman-rho: 0.3808                                                                                                    \n",
      "4863/4863 [==============================] - 767s 158ms/sample - loss: 0.3692 - val_loss: 0.3690\n",
      "Epoch 3/3\n",
      "val_spearman-rho: 0.3828                                                                                                    \n",
      "4863/4863 [==============================] - 767s 158ms/sample - loss: 0.3550 - val_loss: 0.3670\n",
      "Train on 4864 samples, validate on 1215 samples\n",
      "Epoch 1/3\n",
      "val_spearman-rho: 0.3558                                                                                                    \n",
      "4864/4864 [==============================] - 811s 167ms/sample - loss: 0.3959 - val_loss: 0.3778\n",
      "Epoch 2/3\n",
      "val_spearman-rho: 0.3826                                                                                                    \n",
      "4864/4864 [==============================] - 768s 158ms/sample - loss: 0.3668 - val_loss: 0.3713\n",
      "Epoch 3/3\n",
      "val_spearman-rho: 0.383                                                                                                    \n",
      "4864/4864 [==============================] - 767s 158ms/sample - loss: 0.3525 - val_loss: 0.3702\n"
     ]
    }
   ],
   "source": [
    "#X_tr = inputs\n",
    "#y_tr = outputs\n",
    "#K.clear_session()\n",
    "#model = create_model()\n",
    "#optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "#model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "#model.fit(X_tr, y_tr, epochs= 20, batch_size=6,verbose=True)\n",
    "#model.save_weights('berty.h5')\n",
    "\n",
    "\n",
    "gkf = GroupKFold(n_splits = 5).split(X=df_train.question_body, groups=df_train.question_body)\n",
    "valid_preds = []\n",
    "test_preds = []\n",
    "for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
    "    \n",
    "        X_tr = [inputs[i][train_idx] for i in range(len(inputs))]\n",
    "        y_tr = outputs[train_idx]\n",
    "\n",
    "        X_val = [inputs[i][valid_idx] for i in range(len(inputs))]\n",
    "        y_val = outputs[valid_idx]\n",
    "        \n",
    "        K.clear_session()\n",
    "        model = create_model()\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate= 2e-5)\n",
    "        model.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "        model.fit(\n",
    "        X_tr, y_tr, epochs = 3, batch_size= 6, validation_data=(X_val, y_val), verbose=True, \n",
    "        callbacks=[SpearmanRhoCallback(training_data=(X_tr, y_tr), validation_data=(X_val, y_val),\n",
    "                                       patience = 7, model_name=f'best_model3_batch{fold}.h5')]\n",
    "    )\n",
    "        # model.fit(train_inputs, train_outputs, epochs=3, batch_size=6)\n",
    "        #model.save_weights(f'bert-{fold}.h5')\n",
    "      # valid_preds.append(model.predict(valid_inputs))\n",
    "      # test_preds.append(model.predict(test_inputs))\n",
    "        \n",
    "      # rho_val = compute_spearmanr_ignore_nan(valid_outputs, valid_preds[-1])\n",
    "      # print('validation score = ', rho_val)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Process and submit test predictions\n",
    "\n",
    "Average fold predictions, then save as `submission.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sub.iloc[:, 1:] = np.average(test_preds, axis=0) # for weighted average set weights=[...]\n",
    "#\n",
    "#df_sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01d96667dd8046cb8005d3c1d7a1931a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0c5b64599b7e4dc3847c624f68108983": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_5d95e10f6bff4915961cc5d3e3638a74",
        "IPY_MODEL_788247fef65841cf88b5407a64be72c9"
       ],
       "layout": "IPY_MODEL_a7b03b40ea014bb1878e05c0d0f8cee8"
      }
     },
     "27f86a3196da4079991b5dd0ba80f090": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "4e4ec961bc224de2b93e460c73ca5c22": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5d95e10f6bff4915961cc5d3e3638a74": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_01d96667dd8046cb8005d3c1d7a1931a",
       "max": 1.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_cdf6b3670d74414eabb8f5effd6ad691",
       "value": 1.0
      }
     },
     "788247fef65841cf88b5407a64be72c9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_4e4ec961bc224de2b93e460c73ca5c22",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_27f86a3196da4079991b5dd0ba80f090",
       "value": " 6079/? [02:03&lt;00:00, 49.25it/s]"
      }
     },
     "a7b03b40ea014bb1878e05c0d0f8cee8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cdf6b3670d74414eabb8f5effd6ad691": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
